[{"id":"0f9872cd2012b3a0a3679fbaa05b5e0a","title":"install-postgresql","content":"源码编译安装\n\n\n\n\n\n\n\n\n编译根目录地址：/opt\n\n\n\n\n\n\n\n\n\n安装根目录地址：/data\n操作步骤简介\n下载源码\n编译安装\n./configure\nmake\nmake install\n\n\n使用initdb 命令初始化数据库簇（数据库簇：数据库实例管理的系统文件及各个数据库文件的集合。）\n启动数据库实例\n创建一个应用使用的数据库\n\n源码下载解压bash# wget --no-check-certificate  https://ftp.postgresql.org/pub/source/v15.3/postgresql-15.3.tar.gz\n# 华为云镜像地址\ncd /opt\nwget https://repo.huaweicloud.com/postgresql/source/v15.3/postgresql-15.3.tar.gz\n\ntar zxvf postgresql-15.3.tar.gz编译安装依赖组件安装\nbash# 在默认情况下一般要使用数据库中的压缩功能，该功能需要第三方的zlib压缩开发包支持；在Red Hat/CentOS中，该开发包名字后面带“-devel”，而在Ubuntu中是“-dev”\n# CentOS中通过如下命令查找zlib开发包,如下所示，zlib-devel.x86_64为所需开发包\nyum search zlib|grep devel\n    ghc-zlib-devel.x86_64 : Haskell zlib library development files\n    zlib-devel.x86_64 : Header files and libraries for Zlib development\n    zlib-devel.i686 : Header files and libraries for Zlib development\n    zlibrary-devel.x86_64 : Development files for zlibrary\n    genwqe-zlib-devel.i686 : Development files for genwqe-tools\n    genwqe-zlib-devel.x86_64 : Development files for genwqe-tools\n# Ubuntu18.04下，包名为“zlib1g-dev”\napt-get install zlib1g-dev\n# 如果想要方便在psql中使用上下方向键查找历史命令，还需要安装readline开发包，用相似方法可以找到含“readline”和“devel”的包，如图所示，“readline-devel.x86_64”为所需开发包\nyum search readline|grep devel\n    compat-golang-github-chzyer-readline-devel.noarch : Pure Go implementation of GNU Readline-like library\n    golang-gopkg-readline-1-devel.noarch : Pure Go implementation of GNU Readline-like library\n    readline-devel.i686 : Files needed to develop programs which use the readline library\n    readline-devel.x86_64 : Files needed to develop programs which use the readline library\n    tcl-tclreadline-devel.x86_64 : Development files for the tclreadline library\n#在Ubuntu16.04下，为libreadline-dev ？libreadline6-dev\napt-get install libreadline-dev\n\n# --with-perl：加上该选项才能使用Perl语法的PL/Perl过程语言来编写自定义函数。使用该选项需要先安装perl开发包，使用如下命令在Ubuntu或Debian下安装\napt-get install libperl-dev\n# --with-python：加上该选项才能使用Python语法的PL/Python过程语言来编写自定义函数。使用该选项需要先安装python-dev开发包，使用如下命令在Ubuntu或Debian下安装；\napt-get install python-dev\n# 安装gcc套件\n# yum install gcc\napt-get install gcc编译安装\nbashcd /opt/postgresql-15.3\n./configure --prefix=/data/postgresql/15.3 --with-perl --with-python\nmake\nmake install问题记录\nbash# ./configure --with-python时部分情况下可能出现以下编译报错\nchecking Python.h usability... no\nchecking Python.h presence... no\nchecking for Python.h... no\nconfigure: error: header file &lt;Python.h&gt; is required for Python\n# 原因可能是环境中Python过多，编译使用的Python版本未安装开发包安装contrib下工具bashcd /opt/postgresql-15.3/contrib\nmake\nmake installPostgresSQL配置环境变量配置bash# 配置可执行文件的路径\nPGSQL_HOME=/data/postgresql/15.3\nexport PATH=$PGSQL_HOME/bin:$PATH\nexport LD_LIBRARY_PATH=$PGSQL_HOME/lib\n# 配置数据目录\nexport PGDATA=$PGSQL_HOME/pgdata初始化数据库bash# 需要先判断有无pg用户，不使用root初始化数据库，不存在则创建用户postgres；如下为ubuntu创建命令\nadduser postgres\n# 赋权\ncd /data\nchown -R postgres postgresql/\nchmod -R 777 postgresql/\n\n\npostgres@ubuntu:/data/postgresql/15.3$ initdb\nThe files belonging to this database system will be owned by user &quot;postgres&quot;.\nThis user must also own the server process.\n\nThe database cluster will be initialized with locale &quot;en_US.UTF-8&quot;.\nThe default database encoding has accordingly been set to &quot;UTF8&quot;.\nThe default text search configuration will be set to &quot;english&quot;.\n\nData page checksums are disabled.\n\ncreating directory /data/postgresql/15.3/pgdata ... ok\ncreating subdirectories ... ok\nselecting dynamic shared memory implementation ... posix\nselecting default max_connections ... 100\nselecting default shared_buffers ... 128MB\nselecting default time zone ... Asia/Shanghai\ncreating configuration files ... ok\nrunning bootstrap script ... ok\nperforming post-bootstrap initialization ... ok\nsyncing data to disk ... ok\n\ninitdb: warning: enabling &quot;trust&quot; authentication for local connections\ninitdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.\n\nSuccess. You can now start the database server using:\n\n    pg_ctl -D /data/postgresql/15.3/pgdata -l logfile start支持远程访问更新$PGDATA 下pg_hba.conf 文件\nbash# 在文件中加入如下命令：允许任何用户远程连接数据库，连接是需要提供密码\n# pg_hba.conf文件是一个黑白名单的访问控制文件，可以控制允许哪些IP地址的机器访问数据库\nhost    all             all             0/0                     md5更新$PGDATA 下postgresql.conf文件\nbash# 修改监听地址为监听所有地址\nlisten_addresses = &#39;*&#39;更新用户默认密码bash# 登录postgres用户\nsu postgres\n# 执行psql，并更新默认密码并退出\npsql\npostgres=# ALTER USER postgres WITH password &#39;password&#39;;\nALTER ROLE\npostgres=# \\q日志相关配置更新$PGDATA 下postgresql.conf文件\nbash# 打开日志采集\nlogging_collector = on\n# 配置日志目录\nlog_directory = &#39;log&#39;日志切换和覆盖策略：\n方案一：每天生成1个新的日志文件\nbashlog_filename = &#39;postgresql-%Y-%m-%d_%H%M%S.log&#39;\nlog_truncate_on_rotation = off\nlog_rotation_age = 1d\nlog_rotation_size = 0方案二：每当日志写满一定大小，则切换一个日志\nbashlog_filename = &#39;postgresql-%Y-%m-%d_%H%M%S.log&#39;\nlog_truncate_on_rotation = off\nlog_rotation_age = 0\nlog_rotation_size = 10M方案三：只保留最近7天的日志，进行循环覆盖\nbashlog_filename = &#39;postgresql-%a.log&#39;\nlog_truncate_on_rotation = on \nlog_rotation_age = 1d\nlog_rotation_size = 0启停数据库bashpg_ctl start -D $PGDATA\npg_ctl stop -D $PGDATA生产环境配置生成pgtune\n\n\n\n\n\n\n\n\n\npgtune提供了基于经验的生产环境使用的默认配置，如下图\n\ntxt# DB Version: 15\n# OS Type: linux\n# DB Type: web\n# Total Memory (RAM): 16 GB\n# CPUs num: 4\n# Data Storage: hdd\n\nmax_connections = 200\nshared_buffers = 4GB\neffective_cache_size = 12GB\nmaintenance_work_mem = 1GB\ncheckpoint_completion_target = 0.9\nwal_buffers = 16MB\ndefault_statistics_target = 100\nrandom_page_cost = 4\neffective_io_concurrency = 2\nwork_mem = 10485kB\nmin_wal_size = 1GB\nmax_wal_size = 4GB\nmax_worker_processes = 4\nmax_parallel_workers_per_gather = 2\nmax_parallel_workers = 4\nmax_parallel_maintenance_workers = 2配置示例\n点击查看更多\ntxt# -----------------------------\n# PostgreSQL configuration file\n# -----------------------------\n#\n# This file consists of lines of the form:\n#\n#   name = value\n#\n# (The &quot;=&quot; is optional.)  Whitespace may be used.  Comments are introduced with\n# &quot;#&quot; anywhere on a line.  The complete list of parameter names and allowed\n# values can be found in the PostgreSQL documentation.\n#\n# The commented-out settings shown in this file represent the default values.\n# Re-commenting a setting is NOT sufficient to revert it to the default value;\n# you need to reload the server.\n#\n# This file is read on server startup and when the server receives a SIGHUP\n# signal.  If you edit the file on a running system, you have to SIGHUP the\n# server for the changes to take effect, run &quot;pg_ctl reload&quot;, or execute\n# &quot;SELECT pg_reload_conf()&quot;.  Some parameters, which are marked below,\n# require a server shutdown and restart to take effect.\n#\n# Any parameter can also be given as a command-line option to the server, e.g.,\n# &quot;postgres -c log_connections=on&quot;.  Some parameters can be changed at run time\n# with the &quot;SET&quot; SQL command.\n#\n# Memory units:  B  = bytes            Time units:  us  = microseconds\n#                kB = kilobytes                     ms  = milliseconds\n#                MB = megabytes                     s   = seconds\n#                GB = gigabytes                     min = minutes\n#                TB = terabytes                     h   = hours\n#                                                   d   = days\n\n\n#------------------------------------------------------------------------------\n# FILE LOCATIONS\n#------------------------------------------------------------------------------\n\n# The default values of these variables are driven from the -D command-line\n# option or PGDATA environment variable, represented here as ConfigDir.\n\n#data_directory = &#39;ConfigDir&#39;\t\t# use data in another directory\n                    # (change requires restart)\n#hba_file = &#39;ConfigDir/pg_hba.conf&#39;\t# host-based authentication file\n                    # (change requires restart)\n#ident_file = &#39;ConfigDir/pg_ident.conf&#39;\t# ident configuration file\n                    # (change requires restart)\n\n# If external_pid_file is not explicitly set, no extra PID file is written.\n#external_pid_file = &#39;&#39;\t\t\t# write an extra PID file\n                    # (change requires restart)\n\n\n#------------------------------------------------------------------------------\n# CONNECTIONS AND AUTHENTICATION\n#------------------------------------------------------------------------------\n\n# - Connection Settings -\n\n#listen_addresses = &#39;localhost&#39;\t\t# what IP address(es) to listen on;\n                    # comma-separated list of addresses;\n                    # defaults to &#39;localhost&#39;; use &#39;*&#39; for all\n                    # (change requires restart)\n#port = 5432\t\t\t\t# (change requires restart)\n#max_connections = 100\t\t\t# (change requires restart)\n#superuser_reserved_connections = 3\t# (change requires restart)\n#unix_socket_directories = &#39;/tmp&#39;\t# comma-separated list of directories\n                    # (change requires restart)\n#unix_socket_group = &#39;&#39;\t\t\t# (change requires restart)\n#unix_socket_permissions = 0777\t\t# begin with 0 to use octal notation\n                    # (change requires restart)\n#bonjour = off\t\t\t\t# advertise server via Bonjour\n                    # (change requires restart)\n#bonjour_name = &#39;&#39;\t\t\t# defaults to the computer name\n                    # (change requires restart)\n\n# - TCP settings -\n# see &quot;man tcp&quot; for details\n\n#tcp_keepalives_idle = 0\t\t# TCP_KEEPIDLE, in seconds;\n                    # 0 selects the system default\n#tcp_keepalives_interval = 0\t\t# TCP_KEEPINTVL, in seconds;\n                    # 0 selects the system default\n#tcp_keepalives_count = 0\t\t# TCP_KEEPCNT;\n                    # 0 selects the system default\n#tcp_user_timeout = 0\t\t\t# TCP_USER_TIMEOUT, in milliseconds;\n                    # 0 selects the system default\n\n#client_connection_check_interval = 0\t# time between checks for client\n                    # disconnection while running queries;\n                    # 0 for never\n\n# - Authentication -\n\n#authentication_timeout = 1min\t\t# 1s-600s\n#password_encryption = scram-sha-256\t# scram-sha-256 or md5\n#db_user_namespace = off\n\n# GSSAPI using Kerberos\n#krb_server_keyfile = &#39;FILE:$&#123;sysconfdir&#125;/krb5.keytab&#39;\n#krb_caseins_users = off\n\n# - SSL -\n\n#ssl = off\n#ssl_ca_file = &#39;&#39;\n#ssl_cert_file = &#39;server.crt&#39;\n#ssl_crl_file = &#39;&#39;\n#ssl_crl_dir = &#39;&#39;\n#ssl_key_file = &#39;server.key&#39;\n#ssl_ciphers = &#39;HIGH:MEDIUM:+3DES:!aNULL&#39; # allowed SSL ciphers\n#ssl_prefer_server_ciphers = on\n#ssl_ecdh_curve = &#39;prime256v1&#39;\n#ssl_min_protocol_version = &#39;TLSv1.2&#39;\n#ssl_max_protocol_version = &#39;&#39;\n#ssl_dh_params_file = &#39;&#39;\n#ssl_passphrase_command = &#39;&#39;\n#ssl_passphrase_command_supports_reload = off\n\n\n#------------------------------------------------------------------------------\n# RESOURCE USAGE (except WAL)\n#------------------------------------------------------------------------------\n\n# - Memory -\n\n#shared_buffers = 128MB\t\t\t# min 128kB\n                    # (change requires restart)\n#huge_pages = try\t\t\t# on, off, or try\n                    # (change requires restart)\n#huge_page_size = 0\t\t\t# zero for system default\n                    # (change requires restart)\n#temp_buffers = 8MB\t\t\t# min 800kB\n#max_prepared_transactions = 0\t\t# zero disables the feature\n                    # (change requires restart)\n# Caution: it is not advisable to set max_prepared_transactions nonzero unless\n# you actively intend to use prepared transactions.\n#work_mem = 4MB\t\t\t\t# min 64kB\n#hash_mem_multiplier = 2.0\t\t# 1-1000.0 multiplier on hash table work_mem\n#maintenance_work_mem = 64MB\t\t# min 1MB\n#autovacuum_work_mem = -1\t\t# min 1MB, or -1 to use maintenance_work_mem\n#logical_decoding_work_mem = 64MB\t# min 64kB\n#max_stack_depth = 2MB\t\t\t# min 100kB\n#shared_memory_type = mmap\t\t# the default is the first option\n                    # supported by the operating system:\n                    #   mmap\n                    #   sysv\n                    #   windows\n                    # (change requires restart)\ndynamic_shared_memory_type = posix\t# the default is usually the first option\n                    # supported by the operating system:\n                    #   posix\n                    #   sysv\n                    #   windows\n                    #   mmap\n                    # (change requires restart)\n#min_dynamic_shared_memory = 0MB\t# (change requires restart)\n\n# - Disk -\n\n#temp_file_limit = -1\t\t\t# limits per-process temp file space\n                    # in kilobytes, or -1 for no limit\n\n# - Kernel Resources -\n\n#max_files_per_process = 1000\t\t# min 64\n                    # (change requires restart)\n\n# - Cost-Based Vacuum Delay -\n\n#vacuum_cost_delay = 0\t\t\t# 0-100 milliseconds (0 disables)\n#vacuum_cost_page_hit = 1\t\t# 0-10000 credits\n#vacuum_cost_page_miss = 2\t\t# 0-10000 credits\n#vacuum_cost_page_dirty = 20\t\t# 0-10000 credits\n#vacuum_cost_limit = 200\t\t# 1-10000 credits\n\n# - Background Writer -\n\n#bgwriter_delay = 200ms\t\t\t# 10-10000ms between rounds\n#bgwriter_lru_maxpages = 100\t\t# max buffers written/round, 0 disables\n#bgwriter_lru_multiplier = 2.0\t\t# 0-10.0 multiplier on buffers scanned/round\n#bgwriter_flush_after = 512kB\t\t# measured in pages, 0 disables\n\n# - Asynchronous Behavior -\n\n#backend_flush_after = 0\t\t# measured in pages, 0 disables\n#effective_io_concurrency = 1\t\t# 1-1000; 0 disables prefetching\n#maintenance_io_concurrency = 10\t# 1-1000; 0 disables prefetching\n#max_worker_processes = 8\t\t# (change requires restart)\n#max_parallel_workers_per_gather = 2\t# taken from max_parallel_workers\n#max_parallel_maintenance_workers = 2\t# taken from max_parallel_workers\n#max_parallel_workers = 8\t\t# maximum number of max_worker_processes that\n                    # can be used in parallel operations\n#parallel_leader_participation = on\n#old_snapshot_threshold = -1\t\t# 1min-60d; -1 disables; 0 is immediate\n                    # (change requires restart)\n\n\n#------------------------------------------------------------------------------\n# WRITE-AHEAD LOG\n#------------------------------------------------------------------------------\n\n# - Settings -\n\n#wal_level = replica\t\t\t# minimal, replica, or logical\n                    # (change requires restart)\n#fsync = on\t\t\t\t# flush data to disk for crash safety\n                    # (turning this off can cause\n                    # unrecoverable data corruption)\n#synchronous_commit = on\t\t# synchronization level;\n                    # off, local, remote_write, remote_apply, or on\n#wal_sync_method = fsync\t\t# the default is the first option\n                    # supported by the operating system:\n                    #   open_datasync\n                    #   fdatasync (default on Linux and FreeBSD)\n                    #   fsync\n                    #   fsync_writethrough\n                    #   open_sync\n#full_page_writes = on\t\t\t# recover from partial page writes\n#wal_log_hints = off\t\t\t# also do full page writes of non-critical updates\n                    # (change requires restart)\n#wal_compression = off\t\t\t# enables compression of full-page writes;\n                    # off, pglz, lz4, zstd, or on\n#wal_init_zero = on\t\t\t# zero-fill new WAL files\n#wal_recycle = on\t\t\t# recycle WAL files\n#wal_buffers = -1\t\t\t# min 32kB, -1 sets based on shared_buffers\n                    # (change requires restart)\n#wal_writer_delay = 200ms\t\t# 1-10000 milliseconds\n#wal_writer_flush_after = 1MB\t\t# measured in pages, 0 disables\n#wal_skip_threshold = 2MB\n\n#commit_delay = 0\t\t\t# range 0-100000, in microseconds\n#commit_siblings = 5\t\t\t# range 1-1000\n\n# - Checkpoints -\n\n#checkpoint_timeout = 5min\t\t# range 30s-1d\n#checkpoint_completion_target = 0.9\t# checkpoint target duration, 0.0 - 1.0\n#checkpoint_flush_after = 256kB\t\t# measured in pages, 0 disables\n#checkpoint_warning = 30s\t\t# 0 disables\n#max_wal_size = 1GB\n#min_wal_size = 80MB\n\n# - Prefetching during recovery -\n\n#recovery_prefetch = try\t\t# prefetch pages referenced in the WAL?\n#wal_decode_buffer_size = 512kB\t\t# lookahead window used for prefetching\n                    # (change requires restart)\n\n# - Archiving -\n\n#archive_mode = off\t\t# enables archiving; off, on, or always\n                # (change requires restart)\n#archive_library = &#39;&#39;\t\t# library to use to archive a logfile segment\n                # (empty string indicates archive_command should\n                # be used)\n#archive_command = &#39;&#39;\t\t# command to use to archive a logfile segment\n                # placeholders: %p = path of file to archive\n                #               %f = file name only\n                # e.g. &#39;test ! -f /mnt/server/archivedir/%f &amp;&amp; cp %p /mnt/server/archivedir/%f&#39;\n#archive_timeout = 0\t\t# force a logfile segment switch after this\n                # number of seconds; 0 disables\n\n# - Archive Recovery -\n\n# These are only used in recovery mode.\n\n#restore_command = &#39;&#39;\t\t# command to use to restore an archived logfile segment\n                # placeholders: %p = path of file to restore\n                #               %f = file name only\n                # e.g. &#39;cp /mnt/server/archivedir/%f %p&#39;\n#archive_cleanup_command = &#39;&#39;\t# command to execute at every restartpoint\n#recovery_end_command = &#39;&#39;\t# command to execute at completion of recovery\n\n# - Recovery Target -\n\n# Set these only when performing a targeted recovery.\n\n#recovery_target = &#39;&#39;\t\t# &#39;immediate&#39; to end recovery as soon as a\n                                # consistent state is reached\n                # (change requires restart)\n#recovery_target_name = &#39;&#39;\t# the named restore point to which recovery will proceed\n                # (change requires restart)\n#recovery_target_time = &#39;&#39;\t# the time stamp up to which recovery will proceed\n                # (change requires restart)\n#recovery_target_xid = &#39;&#39;\t# the transaction ID up to which recovery will proceed\n                # (change requires restart)\n#recovery_target_lsn = &#39;&#39;\t# the WAL LSN up to which recovery will proceed\n                # (change requires restart)\n#recovery_target_inclusive = on # Specifies whether to stop:\n                # just after the specified recovery target (on)\n                # just before the recovery target (off)\n                # (change requires restart)\n#recovery_target_timeline = &#39;latest&#39;\t# &#39;current&#39;, &#39;latest&#39;, or timeline ID\n                # (change requires restart)\n#recovery_target_action = &#39;pause&#39;\t# &#39;pause&#39;, &#39;promote&#39;, &#39;shutdown&#39;\n                # (change requires restart)\n\n\n#------------------------------------------------------------------------------\n# REPLICATION\n#------------------------------------------------------------------------------\n\n# - Sending Servers -\n\n# Set these on the primary and on any standby that will send replication data.\n\n#max_wal_senders = 10\t\t# max number of walsender processes\n                # (change requires restart)\n#max_replication_slots = 10\t# max number of replication slots\n                # (change requires restart)\n#wal_keep_size = 0\t\t# in megabytes; 0 disables\n#max_slot_wal_keep_size = -1\t# in megabytes; -1 disables\n#wal_sender_timeout = 60s\t# in milliseconds; 0 disables\n#track_commit_timestamp = off\t# collect timestamp of transaction commit\n                # (change requires restart)\n\n# - Primary Server -\n\n# These settings are ignored on a standby server.\n\n#synchronous_standby_names = &#39;&#39;\t# standby servers that provide sync rep\n                # method to choose sync standbys, number of sync standbys,\n                # and comma-separated list of application_name\n                # from standby(s); &#39;*&#39; = all\n#vacuum_defer_cleanup_age = 0\t# number of xacts by which cleanup is delayed\n\n# - Standby Servers -\n\n# These settings are ignored on a primary server.\n\n#primary_conninfo = &#39;&#39;\t\t\t# connection string to sending server\n#primary_slot_name = &#39;&#39;\t\t\t# replication slot on sending server\n#promote_trigger_file = &#39;&#39;\t\t# file name whose presence ends recovery\n#hot_standby = on\t\t\t# &quot;off&quot; disallows queries during recovery\n                    # (change requires restart)\n#max_standby_archive_delay = 30s\t# max delay before canceling queries\n                    # when reading WAL from archive;\n                    # -1 allows indefinite delay\n#max_standby_streaming_delay = 30s\t# max delay before canceling queries\n                    # when reading streaming WAL;\n                    # -1 allows indefinite delay\n#wal_receiver_create_temp_slot = off\t# create temp slot if primary_slot_name\n                    # is not set\n#wal_receiver_status_interval = 10s\t# send replies at least this often\n                    # 0 disables\n#hot_standby_feedback = off\t\t# send info from standby to prevent\n                    # query conflicts\n#wal_receiver_timeout = 60s\t\t# time that receiver waits for\n                    # communication from primary\n                    # in milliseconds; 0 disables\n#wal_retrieve_retry_interval = 5s\t# time to wait before retrying to\n                    # retrieve WAL after a failed attempt\n#recovery_min_apply_delay = 0\t\t# minimum delay for applying changes during recovery\n\n# - Subscribers -\n\n# These settings are ignored on a publisher.\n\n#max_logical_replication_workers = 4\t# taken from max_worker_processes\n                    # (change requires restart)\n#max_sync_workers_per_subscription = 2\t# taken from max_logical_replication_workers\n\n\n#------------------------------------------------------------------------------\n# QUERY TUNING\n#------------------------------------------------------------------------------\n\n# - Planner Method Configuration -\n\n#enable_async_append = on\n#enable_bitmapscan = on\n#enable_gathermerge = on\n#enable_hashagg = on\n#enable_hashjoin = on\n#enable_incremental_sort = on\n#enable_indexscan = on\n#enable_indexonlyscan = on\n#enable_material = on\n#enable_memoize = on\n#enable_mergejoin = on\n#enable_nestloop = on\n#enable_parallel_append = on\n#enable_parallel_hash = on\n#enable_partition_pruning = on\n#enable_partitionwise_join = off\n#enable_partitionwise_aggregate = off\n#enable_seqscan = on\n#enable_sort = on\n#enable_tidscan = on\n\n# - Planner Cost Constants -\n\n#seq_page_cost = 1.0\t\t\t# measured on an arbitrary scale\n#random_page_cost = 4.0\t\t\t# same scale as above\n#cpu_tuple_cost = 0.01\t\t\t# same scale as above\n#cpu_index_tuple_cost = 0.005\t\t# same scale as above\n#cpu_operator_cost = 0.0025\t\t# same scale as above\n#parallel_setup_cost = 1000.0\t# same scale as above\n#parallel_tuple_cost = 0.1\t\t# same scale as above\n#min_parallel_table_scan_size = 8MB\n#min_parallel_index_scan_size = 512kB\n#effective_cache_size = 4GB\n\n#jit_above_cost = 100000\t\t# perform JIT compilation if available\n                    # and query more expensive than this;\n                    # -1 disables\n#jit_inline_above_cost = 500000\t\t# inline small functions if query is\n                    # more expensive than this; -1 disables\n#jit_optimize_above_cost = 500000\t# use expensive JIT optimizations if\n                    # query is more expensive than this;\n                    # -1 disables\n\n# - Genetic Query Optimizer -\n\n#geqo = on\n#geqo_threshold = 12\n#geqo_effort = 5\t\t\t# range 1-10\n#geqo_pool_size = 0\t\t\t# selects default based on effort\n#geqo_generations = 0\t\t\t# selects default based on effort\n#geqo_selection_bias = 2.0\t\t# range 1.5-2.0\n#geqo_seed = 0.0\t\t\t# range 0.0-1.0\n\n# - Other Planner Options -\n\n#default_statistics_target = 100\t# range 1-10000\n#constraint_exclusion = partition\t# on, off, or partition\n#cursor_tuple_fraction = 0.1\t\t# range 0.0-1.0\n#from_collapse_limit = 8\n#jit = on\t\t\t\t# allow JIT compilation\n#join_collapse_limit = 8\t\t# 1 disables collapsing of explicit\n                    # JOIN clauses\n#plan_cache_mode = auto\t\t\t# auto, force_generic_plan or\n                    # force_custom_plan\n#recursive_worktable_factor = 10.0\t# range 0.001-1000000\n\n\n#------------------------------------------------------------------------------\n# REPORTING AND LOGGING\n#------------------------------------------------------------------------------\n\n# - Where to Log -\n\n#log_destination = &#39;stderr&#39;\t\t# Valid values are combinations of\n                    # stderr, csvlog, jsonlog, syslog, and\n                    # eventlog, depending on platform.\n                    # csvlog and jsonlog require\n                    # logging_collector to be on.\n\n# This is used when logging to stderr:\n#logging_collector = off\t\t# Enable capturing of stderr, jsonlog,\n                    # and csvlog into log files. Required\n                    # to be on for csvlogs and jsonlogs.\n                    # (change requires restart)\n\n# These are only used if logging_collector is on:\n#log_directory = &#39;log&#39;\t\t\t# directory where log files are written,\n                    # can be absolute or relative to PGDATA\n#log_filename = &#39;postgresql-%Y-%m-%d_%H%M%S.log&#39;\t# log file name pattern,\n                    # can include strftime() escapes\n#log_file_mode = 0600\t\t\t# creation mode for log files,\n                    # begin with 0 to use octal notation\n#log_rotation_age = 1d\t\t\t# Automatic rotation of logfiles will\n                    # happen after that time.  0 disables.\n#log_rotation_size = 10MB\t\t# Automatic rotation of logfiles will\n                    # happen after that much log output.\n                    # 0 disables.\n#log_truncate_on_rotation = off\t\t# If on, an existing log file with the\n                    # same name as the new log file will be\n                    # truncated rather than appended to.\n                    # But such truncation only occurs on\n                    # time-driven rotation, not on restarts\n                    # or size-driven rotation.  Default is\n                    # off, meaning append to existing files\n                    # in all cases.\n\n# These are relevant when logging to syslog:\n#syslog_facility = &#39;LOCAL0&#39;\n#syslog_ident = &#39;postgres&#39;\n#syslog_sequence_numbers = on\n#syslog_split_messages = on\n\n# This is only relevant when logging to eventlog (Windows):\n# (change requires restart)\n#event_source = &#39;PostgreSQL&#39;\n\n# - When to Log -\n\n#log_min_messages = warning\t\t# values in order of decreasing detail:\n                    #   debug5\n                    #   debug4\n                    #   debug3\n                    #   debug2\n                    #   debug1\n                    #   info\n                    #   notice\n                    #   warning\n                    #   error\n                    #   log\n                    #   fatal\n                    #   panic\n\n#log_min_error_statement = error\t# values in order of decreasing detail:\n                    #   debug5\n                    #   debug4\n                    #   debug3\n                    #   debug2\n                    #   debug1\n                    #   info\n                    #   notice\n                    #   warning\n                    #   error\n                    #   log\n                    #   fatal\n                    #   panic (effectively off)\n\n#log_min_duration_statement = -1\t# -1 is disabled, 0 logs all statements\n                    # and their durations, &gt; 0 logs only\n                    # statements running at least this number\n                    # of milliseconds\n\n#log_min_duration_sample = -1\t\t# -1 is disabled, 0 logs a sample of statements\n                    # and their durations, &gt; 0 logs only a sample of\n                    # statements running at least this number\n                    # of milliseconds;\n                    # sample fraction is determined by log_statement_sample_rate\n\n#log_statement_sample_rate = 1.0\t# fraction of logged statements exceeding\n                    # log_min_duration_sample to be logged;\n                    # 1.0 logs all such statements, 0.0 never logs\n\n\n#log_transaction_sample_rate = 0.0\t# fraction of transactions whose statements\n                    # are logged regardless of their duration; 1.0 logs all\n                    # statements from all transactions, 0.0 never logs\n\n#log_startup_progress_interval = 10s\t# Time between progress updates for\n                    # long-running startup operations.\n                    # 0 disables the feature, &gt; 0 indicates\n                    # the interval in milliseconds.\n\n# - What to Log -\n\n#debug_print_parse = off\n#debug_print_rewritten = off\n#debug_print_plan = off\n#debug_pretty_print = on\n#log_autovacuum_min_duration = 10min\t# log autovacuum activity;\n                    # -1 disables, 0 logs all actions and\n                    # their durations, &gt; 0 logs only\n                    # actions running at least this number\n                    # of milliseconds.\n#log_checkpoints = on\n#log_connections = off\n#log_disconnections = off\n#log_duration = off\n#log_error_verbosity = default\t\t# terse, default, or verbose messages\n#log_hostname = off\n#log_line_prefix = &#39;%m [%p] &#39;\t\t# special values:\n                    #   %a = application name\n                    #   %u = user name\n                    #   %d = database name\n                    #   %r = remote host and port\n                    #   %h = remote host\n                    #   %b = backend type\n                    #   %p = process ID\n                    #   %P = process ID of parallel group leader\n                    #   %t = timestamp without milliseconds\n                    #   %m = timestamp with milliseconds\n                    #   %n = timestamp with milliseconds (as a Unix epoch)\n                    #   %Q = query ID (0 if none or not computed)\n                    #   %i = command tag\n                    #   %e = SQL state\n                    #   %c = session ID\n                    #   %l = session line number\n                    #   %s = session start timestamp\n                    #   %v = virtual transaction ID\n                    #   %x = transaction ID (0 if none)\n                    #   %q = stop here in non-session\n                    #        processes\n                    #   %% = &#39;%&#39;\n                    # e.g. &#39;&lt;%u%%%d&gt; &#39;\n#log_lock_waits = off\t\t\t# log lock waits &gt;= deadlock_timeout\n#log_recovery_conflict_waits = off\t# log standby recovery conflict waits\n                    # &gt;= deadlock_timeout\n#log_parameter_max_length = -1\t\t# when logging statements, limit logged\n                    # bind-parameter values to N bytes;\n                    # -1 means print in full, 0 disables\n#log_parameter_max_length_on_error = 0\t# when logging an error, limit logged\n                    # bind-parameter values to N bytes;\n                    # -1 means print in full, 0 disables\n#log_statement = &#39;none&#39;\t\t\t# none, ddl, mod, all\n#log_replication_commands = off\n#log_temp_files = -1\t\t\t# log temporary files equal or larger\n                    # than the specified size in kilobytes;\n                    # -1 disables, 0 logs all temp files\nlog_timezone = &#39;Asia/Shanghai&#39;\n\n\n#------------------------------------------------------------------------------\n# PROCESS TITLE\n#------------------------------------------------------------------------------\n\n#cluster_name = &#39;&#39;\t\t\t# added to process titles if nonempty\n                    # (change requires restart)\n#update_process_title = on\n\n\n#------------------------------------------------------------------------------\n# STATISTICS\n#------------------------------------------------------------------------------\n\n# - Cumulative Query and Index Statistics -\n\n#track_activities = on\n#track_activity_query_size = 1024\t# (change requires restart)\n#track_counts = on\n#track_io_timing = off\n#track_wal_io_timing = off\n#track_functions = none\t\t\t# none, pl, all\n#stats_fetch_consistency = cache\n\n\n# - Monitoring -\n\n#compute_query_id = auto\n#log_statement_stats = off\n#log_parser_stats = off\n#log_planner_stats = off\n#log_executor_stats = off\n\n\n#------------------------------------------------------------------------------\n# AUTOVACUUM\n#------------------------------------------------------------------------------\n\n#autovacuum = on\t\t\t# Enable autovacuum subprocess?  &#39;on&#39;\n                    # requires track_counts to also be on.\n#autovacuum_max_workers = 3\t\t# max number of autovacuum subprocesses\n                    # (change requires restart)\n#autovacuum_naptime = 1min\t\t# time between autovacuum runs\n#autovacuum_vacuum_threshold = 50\t# min number of row updates before\n                    # vacuum\n#autovacuum_vacuum_insert_threshold = 1000\t# min number of row inserts\n                    # before vacuum; -1 disables insert\n                    # vacuums\n#autovacuum_analyze_threshold = 50\t# min number of row updates before\n                    # analyze\n#autovacuum_vacuum_scale_factor = 0.2\t# fraction of table size before vacuum\n#autovacuum_vacuum_insert_scale_factor = 0.2\t# fraction of inserts over table\n                    # size before insert vacuum\n#autovacuum_analyze_scale_factor = 0.1\t# fraction of table size before analyze\n#autovacuum_freeze_max_age = 200000000\t# maximum XID age before forced vacuum\n                    # (change requires restart)\n#autovacuum_multixact_freeze_max_age = 400000000\t# maximum multixact age\n                    # before forced vacuum\n                    # (change requires restart)\n#autovacuum_vacuum_cost_delay = 2ms\t# default vacuum cost delay for\n                    # autovacuum, in milliseconds;\n                    # -1 means use vacuum_cost_delay\n#autovacuum_vacuum_cost_limit = -1\t# default vacuum cost limit for\n                    # autovacuum, -1 means use\n                    # vacuum_cost_limit\n\n\n#------------------------------------------------------------------------------\n# CLIENT CONNECTION DEFAULTS\n#------------------------------------------------------------------------------\n\n# - Statement Behavior -\n\n#client_min_messages = notice\t\t# values in order of decreasing detail:\n                    #   debug5\n                    #   debug4\n                    #   debug3\n                    #   debug2\n                    #   debug1\n                    #   log\n                    #   notice\n                    #   warning\n                    #   error\n#search_path = &#39;&quot;$user&quot;, public&#39;\t# schema names\n#row_security = on\n#default_table_access_method = &#39;heap&#39;\n#default_tablespace = &#39;&#39;\t\t# a tablespace name, &#39;&#39; uses the default\n#default_toast_compression = &#39;pglz&#39;\t# &#39;pglz&#39; or &#39;lz4&#39;\n#temp_tablespaces = &#39;&#39;\t\t\t# a list of tablespace names, &#39;&#39; uses\n                    # only default tablespace\n#check_function_bodies = on\n#default_transaction_isolation = &#39;read committed&#39;\n#default_transaction_read_only = off\n#default_transaction_deferrable = off\n#session_replication_role = &#39;origin&#39;\n#statement_timeout = 0\t\t\t# in milliseconds, 0 is disabled\n#lock_timeout = 0\t\t\t# in milliseconds, 0 is disabled\n#idle_in_transaction_session_timeout = 0\t# in milliseconds, 0 is disabled\n#idle_session_timeout = 0\t\t# in milliseconds, 0 is disabled\n#vacuum_freeze_table_age = 150000000\n#vacuum_freeze_min_age = 50000000\n#vacuum_failsafe_age = 1600000000\n#vacuum_multixact_freeze_table_age = 150000000\n#vacuum_multixact_freeze_min_age = 5000000\n#vacuum_multixact_failsafe_age = 1600000000\n#bytea_output = &#39;hex&#39;\t\t\t# hex, escape\n#xmlbinary = &#39;base64&#39;\n#xmloption = &#39;content&#39;\n#gin_pending_list_limit = 4MB\n\n# - Locale and Formatting -\n\ndatestyle = &#39;iso, mdy&#39;\n#intervalstyle = &#39;postgres&#39;\ntimezone = &#39;Asia/Shanghai&#39;\n#timezone_abbreviations = &#39;Default&#39;     # Select the set of available time zone\n                    # abbreviations.  Currently, there are\n                    #   Default\n                    #   Australia (historical usage)\n                    #   India\n                    # You can create your own file in\n                    # share/timezonesets/.\n#extra_float_digits = 1\t\t\t# min -15, max 3; any value &gt;0 actually\n                    # selects precise output mode\n#client_encoding = sql_ascii\t\t# actually, defaults to database\n                    # encoding\n\n# These settings are initialized by initdb, but they can be changed.\nlc_messages = &#39;en_US.UTF-8&#39;\t\t\t# locale for system error message\n                    # strings\nlc_monetary = &#39;en_US.UTF-8&#39;\t\t\t# locale for monetary formatting\nlc_numeric = &#39;en_US.UTF-8&#39;\t\t\t# locale for number formatting\nlc_time = &#39;en_US.UTF-8&#39;\t\t\t\t# locale for time formatting\n\n# default configuration for text search\ndefault_text_search_config = &#39;pg_catalog.english&#39;\n\n# - Shared Library Preloading -\n\n#local_preload_libraries = &#39;&#39;\n#session_preload_libraries = &#39;&#39;\n#shared_preload_libraries = &#39;&#39;\t# (change requires restart)\n#jit_provider = &#39;llvmjit&#39;\t\t# JIT library to use\n\n# - Other Defaults -\n\n#dynamic_library_path = &#39;$libdir&#39;\n#gin_fuzzy_search_limit = 0\n\n\n#------------------------------------------------------------------------------\n# LOCK MANAGEMENT\n#------------------------------------------------------------------------------\n\n#deadlock_timeout = 1s\n#max_locks_per_transaction = 64\t\t# min 10\n                    # (change requires restart)\n#max_pred_locks_per_transaction = 64\t# min 10\n                    # (change requires restart)\n#max_pred_locks_per_relation = -2\t# negative values mean\n                    # (max_pred_locks_per_transaction\n                    #  / -max_pred_locks_per_relation) - 1\n#max_pred_locks_per_page = 2            # min 0\n\n\n#------------------------------------------------------------------------------\n# VERSION AND PLATFORM COMPATIBILITY\n#------------------------------------------------------------------------------\n\n# - Previous PostgreSQL Versions -\n\n#array_nulls = on\n#backslash_quote = safe_encoding\t# on, off, or safe_encoding\n#escape_string_warning = on\n#lo_compat_privileges = off\n#quote_all_identifiers = off\n#standard_conforming_strings = on\n#synchronize_seqscans = on\n\n# - Other Platforms and Clients -\n\n#transform_null_equals = off\n\n\n#------------------------------------------------------------------------------\n# ERROR HANDLING\n#------------------------------------------------------------------------------\n\n#exit_on_error = off\t\t\t# terminate session on any error?\n#restart_after_crash = on\t\t# reinitialize after backend crash?\n#data_sync_retry = off\t\t\t# retry or panic on failure to fsync\n                    # data?\n                    # (change requires restart)\n#recovery_init_sync_method = fsync\t# fsync, syncfs (Linux 5.8+)\n\n\n#------------------------------------------------------------------------------\n# CONFIG FILE INCLUDES\n#------------------------------------------------------------------------------\n\n# These options allow settings to be loaded from files other than the\n# default postgresql.conf.  Note that these are directives, not variable\n# assignments, so they can usefully be given more than once.\n\n#include_dir = &#39;...&#39;\t\t\t# include files ending in &#39;.conf&#39; from\n                    # a directory, e.g., &#39;conf.d&#39;\n#include_if_exists = &#39;...&#39;\t\t# include file only if it exists\n#include = &#39;...&#39;\t\t\t# include file\n\n\n#------------------------------------------------------------------------------\n# CUSTOMIZED OPTIONS\n#------------------------------------------------------------------------------\n\n# Add settings for extensions here\n# DB Version: 15\n# OS Type: linux\n# DB Type: web\n# Total Memory (RAM): 16 GB\n# CPUs num: 4\n# Data Storage: hdd\n\nlisten_addresses = &#39;*&#39;\nport = 5432\nmax_connections = 200\nshared_buffers = 4GB\neffective_cache_size = 12GB\nmaintenance_work_mem = 1GB\ncheckpoint_completion_target = 0.9\nwal_buffers = 16MB\ndefault_statistics_target = 100\nrandom_page_cost = 4\neffective_io_concurrency = 2\nwork_mem = 10485kB\nmin_wal_size = 1GB\nmax_wal_size = 4GB\nmax_worker_processes = 4\nmax_parallel_workers_per_gather = 2\nmax_parallel_workers = 4\nmax_parallel_maintenance_workers = 2\n\n","slug":"install-postgresql","date":"2023-11-16T16:01:55.000Z","categories_index":"postgresql","tags_index":"postgresql","author_index":"Rod"},{"id":"395e80b0d84ad157141ca4de3ba9bafc","title":"elasticsearch scripting module","content":"ES脚本模块API兼容性问题在ES的API中是支持脚本的,但是在早期的版本,部分的API版本变化相对比较频繁,因此在不同版本之间是可能存在兼容性问题的,这篇文章主要用于记录部分API的变化.\n\n\n注: 由于项目中仅用到了5.4.x和5.6.x版本的ES,因此该文章只记录了部分版本的变化.\n一. Store Scripts可以使用_scripts端点将脚本存储在集群或者从集群中检索脚本.如果ES启用了安全功能,则必须拥有以下权限才能创建、检索或者删除存储的脚本。\n查看更多安全功能相关信息,请参阅 Security privileges\n1. 5.3.x-5.5.x1.1 存储脚本请求http/_scripts/&#123;id&#125;\nid在存储的脚本中是唯一的。\n\n\n\n如下是一个存储Painless脚本的示例，脚本名为 calculate-store\n\n\nhttpPOST _scripts/calculate-score\n&#123;\n  &quot;script&quot;: &#123;\n    &quot;lang&quot;: &quot;painless&quot;,\n    &quot;code&quot;: &quot;Math.log(_score * 2) + params.my_modifier&quot;\n  &#125;\n&#125;1.2 检索已创建的脚本httpGET _scripts/calculate-score1.3 使用存储的脚本httpGET _search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;script&quot;: &#123;\n      &quot;script&quot;: &#123;\n        &quot;stored&quot;: &quot;calculate-score&quot;,\n        &quot;params&quot;: &#123;\n          &quot;my_modifier&quot;: 2\n        &#125;\n      &#125;\n    &#125;\n  &#125;\n&#125;1.4 删除存储的脚本httpDELETE _scripts/calculate-score2. 5.6.x,6.0.x-6.8.x,7.0.x-7.3.x2.1 存储脚本请求http/_scripts/&#123;id&#125;\nid在存储的脚本中是唯一的。\n\n\n\n如下是一个存储Painless脚本的示例，脚本名为 calculate-store\n\n\nhttpPOST _scripts/calculate-score\n&#123;\n  &quot;script&quot;: &#123;\n    &quot;lang&quot;: &quot;painless&quot;,\n    &quot;source&quot;: &quot;Math.log(_score * 2) + params.my_modifier&quot;\n  &#125;\n&#125;注: 此处与之前的版本不同,发生了变化,code修改为了source\n2.2 检索已创建的脚本httpGET _scripts/calculate-score2.3 使用存储的脚本httpGET _search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;script&quot;: &#123;\n      &quot;script&quot;: &#123;\n        &quot;id&quot;: &quot;calculate-score&quot;,\n        &quot;params&quot;: &#123;\n          &quot;my_modifier&quot;: 2\n        &#125;\n      &#125;\n    &#125;\n  &#125;\n&#125;注: 使用存储的脚本时API发生了变化,stored修改为了id\n2.4 删除存储的脚本httpDELETE _scripts/calculate-score参考文档How to use scripts(master)\n\n","slug":"elasticsearch-scripting-module","date":"2020-06-17T14:43:56.000Z","categories_index":"elasticsearch","tags_index":"elasticsearch","author_index":"Rod"},{"id":"eca93a4fc619ef446eb6c515e09fb668","title":"JanusGraph Index Problem","content":"JanusGraph索引问题整理一. 解决JanusGraph索引更新失败导致数据不一致的场景JanusGraph\n虽然可以支持事务,但其原子性仅保证了更新存储后端时该事务是原子操作,当更新索引后端数据时可能存在失败的场景,此时若未出现其他问题,仅更新索引失败,则事务不会回滚.因此,可能导致索引后端和存储后端数据不一致的场景.\n\n\n\n1. 服务版本列表\n\n\n服务名\nVersion\n\n\n\nJanusGraph\n0.4.x\n\n\nCassandra\n3.11.x\n\n\nElasticSearch\n5.6.x\n\n\n2. JanusGraph索引操作非事务操作JanusGraph提交事务时,若索引操作失败,不会导致事务回滚,因此存在一定的数据不一致的风险,详见代码(Commit indexes部分,仅会打印索引操作的错误日志)\n\njava   // StandardJanusGraph.java\n   public void commit(final Collection&lt;InternalRelation&gt; addedRelations,\n                     final Collection&lt;InternalRelation&gt; deletedRelations, final StandardJanusGraphTx tx) &#123;\n        if (addedRelations.isEmpty() &amp;&amp; deletedRelations.isEmpty()) return;\n        //1. Finalize transaction\n        log.debug(&quot;Saving transaction. Added &#123;&#125;, removed &#123;&#125;&quot;, addedRelations.size(), deletedRelations.size());\n        if (!tx.getConfiguration().hasCommitTime()) tx.getConfiguration().setCommitTime(times.getTime());\n        final Instant txTimestamp = tx.getConfiguration().getCommitTime();\n        final long transactionId = txCounter.incrementAndGet();\n\n        //2. Assign JanusGraphVertex IDs\n        if (!tx.getConfiguration().hasAssignIDsImmediately())\n            idAssigner.assignIDs(addedRelations);\n\n        //3. Commit\n        BackendTransaction mutator = tx.getTxHandle();\n        final boolean acquireLocks = tx.getConfiguration().hasAcquireLocks();\n        final boolean hasTxIsolation = backend.getStoreFeatures().hasTxIsolation();\n        final boolean logTransaction = config.hasLogTransactions() &amp;&amp; !tx.getConfiguration().hasEnabledBatchLoading();\n        final KCVSLog txLog = logTransaction?backend.getSystemTxLog():null;\n        final TransactionLogHeader txLogHeader = new TransactionLogHeader(transactionId,txTimestamp, times);\n        ModificationSummary commitSummary;\n\n        try &#123;\n            //3.1 Log transaction (write-ahead log) if enabled\n            if (logTransaction) &#123;\n                //[FAILURE] Inability to log transaction fails the transaction by escalation since it&#39;s likely due to unavailability of primary\n                //storage backend.\n                Preconditions.checkNotNull(txLog, &quot;Transaction log is null&quot;);\n                txLog.add(txLogHeader.serializeModifications(serializer, LogTxStatus.PRECOMMIT, tx, addedRelations, deletedRelations),txLogHeader.getLogKey());\n            &#125;\n\n            //3.2 Commit schema elements and their associated relations in a separate transaction if backend does not support\n            //    transactional isolation\n            boolean hasSchemaElements = !Iterables.isEmpty(Iterables.filter(deletedRelations,SCHEMA_FILTER))\n                    || !Iterables.isEmpty(Iterables.filter(addedRelations,SCHEMA_FILTER));\n            Preconditions.checkArgument(!hasSchemaElements || (!tx.getConfiguration().hasEnabledBatchLoading() &amp;&amp; acquireLocks),\n                    &quot;Attempting to create schema elements in inconsistent state&quot;);\n\n            if (hasSchemaElements &amp;&amp; !hasTxIsolation) &#123;\n                /*\n                 * On storage without transactional isolation, create separate\n                 * backend transaction for schema aspects to make sure that\n                 * those are persisted prior to and independently of other\n                 * mutations in the tx. If the storage supports transactional\n                 * isolation, then don&#39;t create a separate tx.\n                 */\n                final BackendTransaction schemaMutator = openBackendTransaction(tx);\n\n                try &#123;\n                    //[FAILURE] If the preparation throws an exception abort directly - nothing persisted since batch-loading cannot be enabled for schema elements\n                    commitSummary = prepareCommit(addedRelations,deletedRelations, SCHEMA_FILTER, schemaMutator, tx, acquireLocks);\n                    assert commitSummary.hasModifications &amp;&amp; !commitSummary.has2iModifications;\n                &#125; catch (Throwable e) &#123;\n                    //Roll back schema tx and escalate exception\n                    schemaMutator.rollback();\n                    throw e;\n                &#125;\n\n                try &#123;\n                    schemaMutator.commit();\n                &#125; catch (Throwable e) &#123;\n                    //[FAILURE] Primary persistence failed =&gt; abort and escalate exception, nothing should have been persisted\n                    log.error(&quot;Could not commit transaction [&quot;+transactionId+&quot;] due to storage exception in system-commit&quot;,e);\n                    throw e;\n                &#125;\n            &#125;\n\n            //[FAILURE] Exceptions during preparation here cause the entire transaction to fail on transactional systems\n            //or just the non-system part on others. Nothing has been persisted unless batch-loading\n            commitSummary = prepareCommit(addedRelations,deletedRelations, hasTxIsolation? NO_FILTER : NO_SCHEMA_FILTER, mutator, tx, acquireLocks);\n            if (commitSummary.hasModifications) &#123;\n                String logTxIdentifier = tx.getConfiguration().getLogIdentifier();\n                boolean hasSecondaryPersistence = logTxIdentifier!=null || commitSummary.has2iModifications;\n\n                //1. Commit storage - failures lead to immediate abort\n\n                //1a. Add success message to tx log which will be committed atomically with all transactional changes so that we can recover secondary failures\n                //    This should not throw an exception since the mutations are just cached. If it does, it will be escalated since its critical\n                if (logTransaction) &#123;\n                    txLog.add(txLogHeader.serializePrimary(serializer,\n                                        hasSecondaryPersistence?LogTxStatus.PRIMARY_SUCCESS:LogTxStatus.COMPLETE_SUCCESS),\n                            txLogHeader.getLogKey(),mutator.getTxLogPersistor());\n                &#125;\n\n                try &#123;\n                    mutator.commitStorage();\n                &#125; catch (Throwable e) &#123;\n                    //[FAILURE] If primary storage persistence fails abort directly (only schema could have been persisted)\n                    log.error(&quot;Could not commit transaction [&quot;+transactionId+&quot;] due to storage exception in commit&quot;,e);\n                    throw e;\n                &#125;\n\n                if (hasSecondaryPersistence) &#123;\n                    LogTxStatus status = LogTxStatus.SECONDARY_SUCCESS;\n                    Map&lt;String,Throwable&gt; indexFailures = ImmutableMap.of();\n                    boolean userlogSuccess = true;\n\n                    try &#123;\n                        //2. Commit indexes - [FAILURE] all exceptions are collected and logged but nothing is aborted\n                        indexFailures = mutator.commitIndexes();\n                        if (!indexFailures.isEmpty()) &#123;\n                            status = LogTxStatus.SECONDARY_FAILURE;\n                            for (Map.Entry&lt;String,Throwable&gt; entry : indexFailures.entrySet()) &#123;\n                                log.error(&quot;Error while committing index mutations for transaction [&quot;+transactionId+&quot;] on index: &quot; +entry.getKey(),entry.getValue());\n                            &#125;\n                        &#125;\n                        //3. Log transaction if configured - [FAILURE] is recorded but does not cause exception\n                        if (logTxIdentifier!=null) &#123;\n                            try &#123;\n                                userlogSuccess = false;\n                                final Log userLog = backend.getUserLog(logTxIdentifier);\n                                Future&lt;Message&gt; env = userLog.add(txLogHeader.serializeModifications(serializer, LogTxStatus.USER_LOG, tx, addedRelations, deletedRelations));\n                                if (env.isDone()) &#123;\n                                    try &#123;\n                                        env.get();\n                                    &#125; catch (ExecutionException ex) &#123;\n                                        throw ex.getCause();\n                                    &#125;\n                                &#125;\n                                userlogSuccess=true;\n                            &#125; catch (Throwable e) &#123;\n                                status = LogTxStatus.SECONDARY_FAILURE;\n                                log.error(&quot;Could not user-log committed transaction [&quot;+transactionId+&quot;] to &quot; + logTxIdentifier, e);\n                            &#125;\n                        &#125;\n                    &#125; finally &#123;\n                        if (logTransaction) &#123;\n                            //[FAILURE] An exception here will be logged and not escalated; tx considered success and\n                            // needs to be cleaned up later\n                            try &#123;\n                                txLog.add(txLogHeader.serializeSecondary(serializer,status,indexFailures,userlogSuccess),txLogHeader.getLogKey());\n                            &#125; catch (Throwable e) &#123;\n                                log.error(&quot;Could not tx-log secondary persistence status on transaction [&quot;+transactionId+&quot;]&quot;,e);\n                            &#125;\n                        &#125;\n                    &#125;\n                &#125; else &#123;\n                    //This just closes the transaction since there are no modifications\n                    mutator.commitIndexes();\n                &#125;\n            &#125; else &#123; //Just commit everything at once\n                //[FAILURE] This case only happens when there are no non-system mutations in which case all changes\n                //are already flushed. Hence, an exception here is unlikely and should abort\n                mutator.commit();\n            &#125;\n        &#125; catch (Throwable e) &#123;\n            log.error(&quot;Could not commit transaction [&quot;+transactionId+&quot;] due to exception&quot;,e);\n            try &#123;\n                //Clean up any left-over transaction handles\n                mutator.rollback();\n            &#125; catch (Throwable e2) &#123;\n                log.error(&quot;Could not roll-back transaction [&quot;+transactionId+&quot;] after failure due to exception&quot;,e2);\n            &#125;\n            if (e instanceof RuntimeException) throw (RuntimeException)e;\n            else throw new JanusGraphException(&quot;Unexpected exception&quot;,e);\n        &#125;\n    &#125;\n3. 索引操作执行失败的场景总结3.1 索引并发更新时锁冲突txt154481719 [gremlin-server-exec-6] ERROR org.janusgraph.diskstorage.es.rest.RestElasticSearchClient  - Failed to execute ES query: &#123;type=version_conflict_engine_exception, reason=[byFuzzySearchMixedIndex][12jjc]: version conflict, current version [3] is different than the one provided [2], index_uuid=Zi4DOgwcT8WEjQu-bSJemw, shard=4, index=test__byfuzzysearchmixedindex&#125;该场景的原理比较简单,主要是程序并发修改索引的某一文档时(此时不同请求获取的版本号一致),当其中一个请求修改成功后,版本号改变,其他请求修改时会首先判断版本号是否一致,若不一致,则该次请求失败.\n\n以上场景最简单的解决方案就是重试,但老版本(目前0.5.x以前的版本应该都没有,只确认了一部分)的JanusGraph中Es\n的客户端实现中没有添加重试的配置,最快速的方法时升级JanusGraph版本至0.5.0以上\n\n\nes客户端重试配置的添加可见: issues#1797\n3.2 es某时间段编译大量脚本报错txt154481719 [gremlin-server-exec-6] ERROR org.janusgraph.diskstorage.es.ElasticSearchIndex  - Failed to execute bulk Elasticsearch mutation\njava.io.IOException: Failure(s) in Elasicsearch bulk request: [&#123;type=illegal_argument_exception, reason=failed to execute script, caused_by=&#123;type=general_script_exception, reason=Failed to compile inline script [ctx._source.remove(&quot;*hidden:bp&quot;);ctx._source.remove(&quot;*hidden:bp__STRING&quot;);ctx._source.remove(&quot;*hidden:dp&quot;);ctx._source.remove(&quot;*hidden:dp__STRING&quot;);ctx._source.remove(&quot;*hidden:up&quot;);ctx._source.remove(&quot;*hidden:up__STRING&quot;);ctx._source.remove(&quot;*hidden:nc&quot;);ctx._source.remove(&quot;*hidden:nc__STRING&quot;);ctx._source.remove(&quot;*hidden:tu&quot;);ctx._source.remove(&quot;*hidden:tu__STRING&quot;);] using lang [painless], caused_by=&#123;type=circuit_breaking_exception, reason=[script] Too many dynamic script compilations within one minute, max: [15/min]; please use on-disk, indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_per_minute] setting, bytes_wanted=0, bytes_limit=0&#125;&#125;&#125;]\n    at org.janusgraph.diskstorage.es.rest.RestElasticSearchClient.bulkRequest(RestElasticSearchClient.java:258)\n    at org.janusgraph.diskstorage.es.ElasticSearchIndex.mutate(ElasticSearchIndex.java:601)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction$1.call(IndexTransaction.java:160)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction$1.call(IndexTransaction.java:157)\n    at org.janusgraph.diskstorage.util.BackendOperation.executeDirect(BackendOperation.java:69)\n    at org.janusgraph.diskstorage.util.BackendOperation.execute(BackendOperation.java:55)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction.flushInternal(IndexTransaction.java:157)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction.commit(IndexTransaction.java:138)\n    at org.janusgraph.diskstorage.BackendTransaction.commitIndexes(BackendTransaction.java:141)\n    at org.janusgraph.graphdb.database.StandardJanusGraph.commit(StandardJanusGraph.java:765)\n    at org.janusgraph.graphdb.transaction.StandardJanusGraphTx.commit(StandardJanusGraphTx.java:1374)\n    at org.janusgraph.graphdb.tinkerpop.JanusGraphBlueprintsGraph$GraphTransaction.doCommit(JanusGraphBlueprintsGraph.java:272)\n    at org.apache.tinkerpop.gremlin.structure.util.AbstractTransaction.commit(AbstractTransaction.java:105)\n    at org.apache.tinkerpop.gremlin.structure.Transaction$commit$4.call(Unknown Source)\n    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48)\n    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113)\n    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)\n    at Script27489.run(Script27489.groovy:1)\n    at org.apache.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:843)\n    at org.apache.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:548)\n    at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:233)\n    at org.apache.tinkerpop.gremlin.groovy.engine.ScriptEngines.eval(ScriptEngines.java:120)\n    at org.apache.tinkerpop.gremlin.groovy.engine.GremlinExecutor.lambda$eval$0(GremlinExecutor.java:290)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)该错误主要由于某段时间编译了大量的脚本,导致触发了ES的保护机制,拒绝了部分脚本的编译请求.\n最简单的解决方案就是调大限制,但是不可避免的会给ES集群造成压力,因此,不采用当前方案;另一种方案就是想办法从根本上解决大量脚本编译的问题.\n在我们的使用场景下,通常在大量删除节点的属性(相关属性创建了MixedIndex索引)时会出现这个问题,对具体的场景进行分析发现,当我们在删除节点属性时,JanusGraph\n会向ES发送fields删除的脚本.首先,这部分脚本没有进行参数化;其次,JanusGraph\n构造脚本时,属性列表顺序并不是有序的,因此,虽然我们的使用场景中每次删除的都是不同节点上的相同属性,但可能每次执行删除操作时构造的脚本都不同,发送至ES后,ES每次都需重新编译,所以触发了这个问题.\n\n在JanusGraph0.5.x版本后,修复了这个问题,删除Fields时,使用的是参数化的语句,且脚本已经提前提交至ES进行了编译.对于该问题的优化相关的解释可参考\nElasticsearch Painless script编程或ElasticSearch-DynamicScript使用总结\n但是很可惜,我们的使用场景中无法升级JanusGraph版本至0.5.x,因为该版本以后JanusGraph不在支持ES5,我们的项目还不能升级ES\n版本,因此只能对之前的JanusGraph的代码进行修改.相关的修改如下:\n\n思路(参考JanusGraph0.5.x版本中的修复方案):\n\n\n服务启动时,尝试存储可参数化的脚本至ES集群中.\n使用脚本时,通过参数化的方式发送请求至ES集群,可以最有效的防止频繁进行脚本的编译.\n\n\n代码修改时主要关注ElasticSearchIndex即可(其他已省略),可通过该类逐步完成所有修改.\n\njava\n    // 添加需要执行的脚本, ElasticSearchIndex.java\n    private static final String PARAMETERIZED_DELETION_SCRIPT = parameterizedScriptPrepare(&quot;&quot;,\n        &quot;for (field in params.fields) &#123;&quot;,\n        &quot;    if (field.cardinality == &#39;SINGLE&#39;) &#123;&quot;,\n        &quot;        ctx._source.remove(field.name);&quot;,\n        &quot;    &#125; else if (ctx._source.containsKey(field.name)) &#123;&quot;,\n        &quot;        def fieldIndex = ctx._source[field.name].indexOf(field.value);&quot;,\n        &quot;        if (fieldIndex &gt;= 0 &amp;&amp; fieldIndex &lt; ctx._source[field.name].size()) &#123;&quot;,\n        &quot;            ctx._source[field.name].remove(fieldIndex);&quot;,\n        &quot;        &#125;&quot;,\n        &quot;    &#125;&quot;,\n        &quot;&#125;&quot;);\n\n    private static final String PARAMETERIZED_ADDITION_SCRIPT = parameterizedScriptPrepare(&quot;&quot;,\n        &quot;for (field in params.fields) &#123;&quot;,\n        &quot;    if (ctx._source[field.name] == null) &#123;&quot;,\n        &quot;        ctx._source[field.name] = [];&quot;,\n        &quot;    &#125;&quot;,\n        &quot;    if (field.cardinality != &#39;SET&#39; || ctx._source[field.name].indexOf(field.value) == -1) &#123;&quot;,\n        &quot;        ctx._source[field.name].add(field.value);&quot;,\n        &quot;    &#125;&quot;,\n        &quot;&#125;&quot;);\n...\n    // 构造方法是重点,此处会将painless脚本通过_script端点存储至ES集群中\n    public ElasticSearchIndex(Configuration config) throws BackendException &#123;\n        indexName = config.get(INDEX_NAME);\n        parameterizedAdditionScriptId = generateScriptId(&quot;add&quot;);\n        parameterizedDeletionScriptId = generateScriptId(&quot;del&quot;);\n        useAllField = config.get(USE_ALL_FIELD);\n        useExternalMappings = config.get(USE_EXTERNAL_MAPPINGS);\n        allowMappingUpdate = config.get(ALLOW_MAPPING_UPDATE);\n        createSleep = config.get(CREATE_SLEEP);\n        ingestPipelines = config.getSubset(ES_INGEST_PIPELINES);\n        final ElasticSearchSetup.Connection c = interfaceConfiguration(config);\n        client = c.getClient();\n\n        batchSize = config.get(INDEX_MAX_RESULT_SET_SIZE);\n        log.debug(&quot;Configured ES query nb result by query to &#123;&#125;&quot;, batchSize);\n\n        switch (client.getMajorVersion()) &#123;\n            case FIVE:\n                compat = new ES5Compat();\n                break;\n            case SIX:\n                compat = new ES6Compat();\n                break;\n            default:\n                throw new PermanentBackendException(&quot;Unsupported Elasticsearch version: &quot; + client.getMajorVersion());\n        &#125;\n\n        try &#123;\n            client.clusterHealthRequest(config.get(HEALTH_REQUEST_TIMEOUT));\n        &#125; catch (final IOException e) &#123;\n            throw new PermanentBackendException(e.getMessage(), e);\n        &#125;\n        if (!config.has(USE_DEPRECATED_MULTITYPE_INDEX) &amp;&amp; client.isIndex(indexName)) &#123;\n            // upgrade scenario where multitype index was the default behavior\n            useMultitypeIndex = true;\n        &#125; else &#123;\n            useMultitypeIndex = config.get(USE_DEPRECATED_MULTITYPE_INDEX);\n            Preconditions.checkArgument(!useMultitypeIndex || !client.isAlias(indexName),\n                    &quot;The key &#39;&quot; + USE_DEPRECATED_MULTITYPE_INDEX\n                    + &quot;&#39; cannot be true when existing index is split.&quot;);\n            Preconditions.checkArgument(useMultitypeIndex || !client.isIndex(indexName),\n                    &quot;The key &#39;&quot; + USE_DEPRECATED_MULTITYPE_INDEX\n                    + &quot;&#39; cannot be false when existing index contains multiple types.&quot;);\n        &#125;\n        indexSetting = new HashMap&lt;&gt;();\n\n        ElasticSearchSetup.applySettingsFromJanusGraphConf(indexSetting, config);\n        indexSetting.put(&quot;index.max_result_window&quot;, Integer.MAX_VALUE);\n        \n        // 通过该方法在服务启动时存储脚本,注意存储脚本时需要注意版本兼容性问题\n        setupStoredScripts();\n    &#125;\n\n    // 该方法是执行请求的路口,在该方法中需要将原脚本修改为参数化脚本,防止触发ES保护机制\n    @Override\n    public void mutate(Map&lt;String, Map&lt;String, IndexMutation&gt;&gt; mutations, KeyInformation.IndexRetriever information,\n                       BaseTransaction tx) throws BackendException &#123;\n        final List&lt;ElasticSearchMutation&gt; requests = new ArrayList&lt;&gt;();\n        try &#123;\n            for (final Map.Entry&lt;String, Map&lt;String, IndexMutation&gt;&gt; stores : mutations.entrySet()) &#123;\n                final List&lt;ElasticSearchMutation&gt; requestByStore = new ArrayList&lt;&gt;();\n                final String storeName = stores.getKey();\n                final String indexStoreName = getIndexStoreName(storeName);\n                for (final Map.Entry&lt;String, IndexMutation&gt; entry : stores.getValue().entrySet()) &#123;\n                    final String documentId = entry.getKey();\n                    final IndexMutation mutation = entry.getValue();\n                    assert mutation.isConsolidated();\n                    Preconditions.checkArgument(!(mutation.isNew() &amp;&amp; mutation.isDeleted()));\n                    Preconditions.checkArgument(!mutation.isNew() || !mutation.hasDeletions());\n                    Preconditions.checkArgument(!mutation.isDeleted() || !mutation.hasAdditions());\n                    //Deletions first\n                    if (mutation.hasDeletions()) &#123;\n                        if (mutation.isDeleted()) &#123;\n                            log.trace(&quot;Deleting entire document &#123;&#125;&quot;, documentId);\n                            requestByStore.add(ElasticSearchMutation.createDeleteRequest(indexStoreName, storeName,\n                                    documentId));\n                        &#125; else &#123;\n//                            final String script = getDeletionScript(information, storeName, mutation);\n//                            final Map&lt;String,Object&gt; doc = compat.prepareScript(script).build();\n                            List&lt;Map&lt;String, Object&gt;&gt; params = getParameters(information.get(storeName),\n                                mutation.getDeletions(), true);\n                            Map doc = compat.prepareStoredScript(parameterizedDeletionScriptId, params).build();\n                            log.trace(&quot;Deletion script &#123;&#125; with params &#123;&#125;&quot;, PARAMETERIZED_DELETION_SCRIPT, params);\n                            requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n                                    documentId, doc));\n                        &#125;\n                    &#125;\n                    if (mutation.hasAdditions()) &#123;\n                        if (mutation.isNew()) &#123; //Index\n                            log.trace(&quot;Adding entire document &#123;&#125;&quot;, documentId);\n                            final Map&lt;String, Object&gt; source = getNewDocument(mutation.getAdditions(),\n                                    information.get(storeName));\n                            requestByStore.add(ElasticSearchMutation.createIndexRequest(indexStoreName, storeName,\n                                    documentId, source));\n                        &#125; else &#123;\n                            final Map upsert;\n                            if (!mutation.hasDeletions()) &#123;\n                                upsert = getNewDocument(mutation.getAdditions(), information.get(storeName));\n                            &#125; else &#123;\n                                upsert = null;\n                            &#125;\n\n                            List&lt;Map&lt;String, Object&gt;&gt; params = getParameters(information.get(storeName),\n                                mutation.getAdditions(), false, Cardinality.SINGLE);\n                            if (!params.isEmpty()) &#123;\n                                ImmutableMap.Builder builder = compat.prepareStoredScript(parameterizedAdditionScriptId, params);\n                                requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n                                    documentId, builder, upsert));\n                                log.trace(&quot;Adding script &#123;&#125; with params &#123;&#125;&quot;, PARAMETERIZED_ADDITION_SCRIPT, params);\n                            &#125;\n//                            final String inline = getAdditionScript(information, storeName, mutation);\n//                            if (!inline.isEmpty()) &#123;\n//                                final ImmutableMap.Builder builder = compat.prepareScript(inline);\n//                                requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n//                                        documentId, builder, upsert));\n//                                log.trace(&quot;Adding script &#123;&#125;&quot;, inline);\n//                            &#125;\n\n                            final Map&lt;String, Object&gt; doc = getAdditionDoc(information, storeName, mutation);\n                            if (!doc.isEmpty()) &#123;\n                                final ImmutableMap.Builder builder = ImmutableMap.builder().put(ES_DOC_KEY, doc);\n                                requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n                                        documentId, builder, upsert));\n                                log.trace(&quot;Adding update &#123;&#125;&quot;, doc);\n                            &#125;\n                        &#125;\n                    &#125;\n                &#125;\n                if (!requestByStore.isEmpty() &amp;&amp; ingestPipelines.containsKey(storeName)) &#123;\n                    client.bulkRequest(requestByStore, String.valueOf(ingestPipelines.get(storeName)));\n                &#125; else if (!requestByStore.isEmpty()) &#123;\n                    requests.addAll(requestByStore);\n                &#125;\n            &#125;\n            if (!requests.isEmpty()) &#123;\n                client.bulkRequest(requests, null);\n            &#125;\n        &#125; catch (final Exception e) &#123;\n            log.error(&quot;Failed to execute bulk Elasticsearch mutation&quot;, e);\n            throw convert(e);\n        &#125;\n    &#125;由于存储脚本的API在不同的版本中存在变化,因此需要对此部分做兼容处理,相关变化详见elasticsearch scripting module\n","slug":"janusgraph-index-problem","date":"2020-06-14T06:00:00.000Z","categories_index":"janusgraph,index","tags_index":"janusgraph,index","author_index":"Rod"}]