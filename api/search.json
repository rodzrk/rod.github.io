[{"id":"395e80b0d84ad157141ca4de3ba9bafc","title":"elasticsearch scripting module","content":"ES脚本模块API兼容性问题在ES的API中是支持脚本的,但是在早期的版本,部分的API版本变化相对比较频繁,因此在不同版本之间是可能存在兼容性问题的,这篇文章主要用于记录部分API的变化.\n\n\n注: 由于项目中仅用到了5.4.x和5.6.x版本的ES,因此该文章只记录了部分版本的变化.\n一. Store Scripts可以使用_scripts端点将脚本存储在集群或者从集群中检索脚本.如果ES启用了安全功能,则必须拥有以下权限才能创建、检索或者删除存储的脚本。\n查看更多安全功能相关信息,请参阅 Security privileges\n1. 5.3.x-5.5.x1.1 存储脚本请求http/_scripts/&#123;id&#125;\nid在存储的脚本中是唯一的。\n\n\n\n如下是一个存储Painless脚本的示例，脚本名为 calculate-store\n\n\nhttpPOST _scripts/calculate-score\n&#123;\n  &quot;script&quot;: &#123;\n    &quot;lang&quot;: &quot;painless&quot;,\n    &quot;code&quot;: &quot;Math.log(_score * 2) + params.my_modifier&quot;\n  &#125;\n&#125;1.2 检索已创建的脚本httpGET _scripts/calculate-score1.3 使用存储的脚本httpGET _search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;script&quot;: &#123;\n      &quot;script&quot;: &#123;\n        &quot;stored&quot;: &quot;calculate-score&quot;,\n        &quot;params&quot;: &#123;\n          &quot;my_modifier&quot;: 2\n        &#125;\n      &#125;\n    &#125;\n  &#125;\n&#125;1.4 删除存储的脚本httpDELETE _scripts/calculate-score2. 5.6.x,6.0.x-6.8.x,7.0.x-7.3.x2.1 存储脚本请求http/_scripts/&#123;id&#125;\nid在存储的脚本中是唯一的。\n\n\n\n如下是一个存储Painless脚本的示例，脚本名为 calculate-store\n\n\nhttpPOST _scripts/calculate-score\n&#123;\n  &quot;script&quot;: &#123;\n    &quot;lang&quot;: &quot;painless&quot;,\n    &quot;source&quot;: &quot;Math.log(_score * 2) + params.my_modifier&quot;\n  &#125;\n&#125;注: 此处与之前的版本不同,发生了变化,code修改为了source\n2.2 检索已创建的脚本httpGET _scripts/calculate-score2.3 使用存储的脚本httpGET _search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;script&quot;: &#123;\n      &quot;script&quot;: &#123;\n        &quot;id&quot;: &quot;calculate-score&quot;,\n        &quot;params&quot;: &#123;\n          &quot;my_modifier&quot;: 2\n        &#125;\n      &#125;\n    &#125;\n  &#125;\n&#125;注: 使用存储的脚本时API发生了变化,stored修改为了id\n2.4 删除存储的脚本httpDELETE _scripts/calculate-score参考文档How to use scripts(master)\n\n","slug":"elasticsearch-scripting-module","date":"2020-06-17T14:43:56.000Z","categories_index":"elasticsearch","tags_index":"elasticsearch","author_index":"Rod"},{"id":"eca93a4fc619ef446eb6c515e09fb668","title":"JanusGraph Index Problem","content":"JanusGraph索引问题整理一. 解决JanusGraph索引更新失败导致数据不一致的场景JanusGraph\n虽然可以支持事务,但其原子性仅保证了更新存储后端时该事务是原子操作,当更新索引后端数据时可能存在失败的场景,此时若未出现其他问题,仅更新索引失败,则事务不会回滚.因此,可能导致索引后端和存储后端数据不一致的场景.\n\n\n\n1. 服务版本列表\n\n\n服务名\nVersion\n\n\n\nJanusGraph\n0.4.x\n\n\nCassandra\n3.11.x\n\n\nElasticSearch\n5.6.x\n\n\n2. JanusGraph索引操作非事务操作JanusGraph提交事务时,若索引操作失败,不会导致事务回滚,因此存在一定的数据不一致的风险,详见代码(Commit indexes部分,仅会打印索引操作的错误日志)\n\njava   // StandardJanusGraph.java\n   public void commit(final Collection&lt;InternalRelation&gt; addedRelations,\n                     final Collection&lt;InternalRelation&gt; deletedRelations, final StandardJanusGraphTx tx) &#123;\n        if (addedRelations.isEmpty() &amp;&amp; deletedRelations.isEmpty()) return;\n        //1. Finalize transaction\n        log.debug(&quot;Saving transaction. Added &#123;&#125;, removed &#123;&#125;&quot;, addedRelations.size(), deletedRelations.size());\n        if (!tx.getConfiguration().hasCommitTime()) tx.getConfiguration().setCommitTime(times.getTime());\n        final Instant txTimestamp = tx.getConfiguration().getCommitTime();\n        final long transactionId = txCounter.incrementAndGet();\n\n        //2. Assign JanusGraphVertex IDs\n        if (!tx.getConfiguration().hasAssignIDsImmediately())\n            idAssigner.assignIDs(addedRelations);\n\n        //3. Commit\n        BackendTransaction mutator = tx.getTxHandle();\n        final boolean acquireLocks = tx.getConfiguration().hasAcquireLocks();\n        final boolean hasTxIsolation = backend.getStoreFeatures().hasTxIsolation();\n        final boolean logTransaction = config.hasLogTransactions() &amp;&amp; !tx.getConfiguration().hasEnabledBatchLoading();\n        final KCVSLog txLog = logTransaction?backend.getSystemTxLog():null;\n        final TransactionLogHeader txLogHeader = new TransactionLogHeader(transactionId,txTimestamp, times);\n        ModificationSummary commitSummary;\n\n        try &#123;\n            //3.1 Log transaction (write-ahead log) if enabled\n            if (logTransaction) &#123;\n                //[FAILURE] Inability to log transaction fails the transaction by escalation since it&#39;s likely due to unavailability of primary\n                //storage backend.\n                Preconditions.checkNotNull(txLog, &quot;Transaction log is null&quot;);\n                txLog.add(txLogHeader.serializeModifications(serializer, LogTxStatus.PRECOMMIT, tx, addedRelations, deletedRelations),txLogHeader.getLogKey());\n            &#125;\n\n            //3.2 Commit schema elements and their associated relations in a separate transaction if backend does not support\n            //    transactional isolation\n            boolean hasSchemaElements = !Iterables.isEmpty(Iterables.filter(deletedRelations,SCHEMA_FILTER))\n                    || !Iterables.isEmpty(Iterables.filter(addedRelations,SCHEMA_FILTER));\n            Preconditions.checkArgument(!hasSchemaElements || (!tx.getConfiguration().hasEnabledBatchLoading() &amp;&amp; acquireLocks),\n                    &quot;Attempting to create schema elements in inconsistent state&quot;);\n\n            if (hasSchemaElements &amp;&amp; !hasTxIsolation) &#123;\n                /*\n                 * On storage without transactional isolation, create separate\n                 * backend transaction for schema aspects to make sure that\n                 * those are persisted prior to and independently of other\n                 * mutations in the tx. If the storage supports transactional\n                 * isolation, then don&#39;t create a separate tx.\n                 */\n                final BackendTransaction schemaMutator = openBackendTransaction(tx);\n\n                try &#123;\n                    //[FAILURE] If the preparation throws an exception abort directly - nothing persisted since batch-loading cannot be enabled for schema elements\n                    commitSummary = prepareCommit(addedRelations,deletedRelations, SCHEMA_FILTER, schemaMutator, tx, acquireLocks);\n                    assert commitSummary.hasModifications &amp;&amp; !commitSummary.has2iModifications;\n                &#125; catch (Throwable e) &#123;\n                    //Roll back schema tx and escalate exception\n                    schemaMutator.rollback();\n                    throw e;\n                &#125;\n\n                try &#123;\n                    schemaMutator.commit();\n                &#125; catch (Throwable e) &#123;\n                    //[FAILURE] Primary persistence failed =&gt; abort and escalate exception, nothing should have been persisted\n                    log.error(&quot;Could not commit transaction [&quot;+transactionId+&quot;] due to storage exception in system-commit&quot;,e);\n                    throw e;\n                &#125;\n            &#125;\n\n            //[FAILURE] Exceptions during preparation here cause the entire transaction to fail on transactional systems\n            //or just the non-system part on others. Nothing has been persisted unless batch-loading\n            commitSummary = prepareCommit(addedRelations,deletedRelations, hasTxIsolation? NO_FILTER : NO_SCHEMA_FILTER, mutator, tx, acquireLocks);\n            if (commitSummary.hasModifications) &#123;\n                String logTxIdentifier = tx.getConfiguration().getLogIdentifier();\n                boolean hasSecondaryPersistence = logTxIdentifier!=null || commitSummary.has2iModifications;\n\n                //1. Commit storage - failures lead to immediate abort\n\n                //1a. Add success message to tx log which will be committed atomically with all transactional changes so that we can recover secondary failures\n                //    This should not throw an exception since the mutations are just cached. If it does, it will be escalated since its critical\n                if (logTransaction) &#123;\n                    txLog.add(txLogHeader.serializePrimary(serializer,\n                                        hasSecondaryPersistence?LogTxStatus.PRIMARY_SUCCESS:LogTxStatus.COMPLETE_SUCCESS),\n                            txLogHeader.getLogKey(),mutator.getTxLogPersistor());\n                &#125;\n\n                try &#123;\n                    mutator.commitStorage();\n                &#125; catch (Throwable e) &#123;\n                    //[FAILURE] If primary storage persistence fails abort directly (only schema could have been persisted)\n                    log.error(&quot;Could not commit transaction [&quot;+transactionId+&quot;] due to storage exception in commit&quot;,e);\n                    throw e;\n                &#125;\n\n                if (hasSecondaryPersistence) &#123;\n                    LogTxStatus status = LogTxStatus.SECONDARY_SUCCESS;\n                    Map&lt;String,Throwable&gt; indexFailures = ImmutableMap.of();\n                    boolean userlogSuccess = true;\n\n                    try &#123;\n                        //2. Commit indexes - [FAILURE] all exceptions are collected and logged but nothing is aborted\n                        indexFailures = mutator.commitIndexes();\n                        if (!indexFailures.isEmpty()) &#123;\n                            status = LogTxStatus.SECONDARY_FAILURE;\n                            for (Map.Entry&lt;String,Throwable&gt; entry : indexFailures.entrySet()) &#123;\n                                log.error(&quot;Error while committing index mutations for transaction [&quot;+transactionId+&quot;] on index: &quot; +entry.getKey(),entry.getValue());\n                            &#125;\n                        &#125;\n                        //3. Log transaction if configured - [FAILURE] is recorded but does not cause exception\n                        if (logTxIdentifier!=null) &#123;\n                            try &#123;\n                                userlogSuccess = false;\n                                final Log userLog = backend.getUserLog(logTxIdentifier);\n                                Future&lt;Message&gt; env = userLog.add(txLogHeader.serializeModifications(serializer, LogTxStatus.USER_LOG, tx, addedRelations, deletedRelations));\n                                if (env.isDone()) &#123;\n                                    try &#123;\n                                        env.get();\n                                    &#125; catch (ExecutionException ex) &#123;\n                                        throw ex.getCause();\n                                    &#125;\n                                &#125;\n                                userlogSuccess=true;\n                            &#125; catch (Throwable e) &#123;\n                                status = LogTxStatus.SECONDARY_FAILURE;\n                                log.error(&quot;Could not user-log committed transaction [&quot;+transactionId+&quot;] to &quot; + logTxIdentifier, e);\n                            &#125;\n                        &#125;\n                    &#125; finally &#123;\n                        if (logTransaction) &#123;\n                            //[FAILURE] An exception here will be logged and not escalated; tx considered success and\n                            // needs to be cleaned up later\n                            try &#123;\n                                txLog.add(txLogHeader.serializeSecondary(serializer,status,indexFailures,userlogSuccess),txLogHeader.getLogKey());\n                            &#125; catch (Throwable e) &#123;\n                                log.error(&quot;Could not tx-log secondary persistence status on transaction [&quot;+transactionId+&quot;]&quot;,e);\n                            &#125;\n                        &#125;\n                    &#125;\n                &#125; else &#123;\n                    //This just closes the transaction since there are no modifications\n                    mutator.commitIndexes();\n                &#125;\n            &#125; else &#123; //Just commit everything at once\n                //[FAILURE] This case only happens when there are no non-system mutations in which case all changes\n                //are already flushed. Hence, an exception here is unlikely and should abort\n                mutator.commit();\n            &#125;\n        &#125; catch (Throwable e) &#123;\n            log.error(&quot;Could not commit transaction [&quot;+transactionId+&quot;] due to exception&quot;,e);\n            try &#123;\n                //Clean up any left-over transaction handles\n                mutator.rollback();\n            &#125; catch (Throwable e2) &#123;\n                log.error(&quot;Could not roll-back transaction [&quot;+transactionId+&quot;] after failure due to exception&quot;,e2);\n            &#125;\n            if (e instanceof RuntimeException) throw (RuntimeException)e;\n            else throw new JanusGraphException(&quot;Unexpected exception&quot;,e);\n        &#125;\n    &#125;\n3. 索引操作执行失败的场景总结3.1 索引并发更新时锁冲突txt154481719 [gremlin-server-exec-6] ERROR org.janusgraph.diskstorage.es.rest.RestElasticSearchClient  - Failed to execute ES query: &#123;type=version_conflict_engine_exception, reason=[byFuzzySearchMixedIndex][12jjc]: version conflict, current version [3] is different than the one provided [2], index_uuid=Zi4DOgwcT8WEjQu-bSJemw, shard=4, index=test__byfuzzysearchmixedindex&#125;该场景的原理比较简单,主要是程序并发修改索引的某一文档时(此时不同请求获取的版本号一致),当其中一个请求修改成功后,版本号改变,其他请求修改时会首先判断版本号是否一致,若不一致,则该次请求失败.\n\n以上场景最简单的解决方案就是重试,但老版本(目前0.5.x以前的版本应该都没有,只确认了一部分)的JanusGraph中Es\n的客户端实现中没有添加重试的配置,最快速的方法时升级JanusGraph版本至0.5.0以上\n\n\nes客户端重试配置的添加可见: issues#1797\n3.2 es某时间段编译大量脚本报错txt154481719 [gremlin-server-exec-6] ERROR org.janusgraph.diskstorage.es.ElasticSearchIndex  - Failed to execute bulk Elasticsearch mutation\njava.io.IOException: Failure(s) in Elasicsearch bulk request: [&#123;type=illegal_argument_exception, reason=failed to execute script, caused_by=&#123;type=general_script_exception, reason=Failed to compile inline script [ctx._source.remove(&quot;*hidden:bp&quot;);ctx._source.remove(&quot;*hidden:bp__STRING&quot;);ctx._source.remove(&quot;*hidden:dp&quot;);ctx._source.remove(&quot;*hidden:dp__STRING&quot;);ctx._source.remove(&quot;*hidden:up&quot;);ctx._source.remove(&quot;*hidden:up__STRING&quot;);ctx._source.remove(&quot;*hidden:nc&quot;);ctx._source.remove(&quot;*hidden:nc__STRING&quot;);ctx._source.remove(&quot;*hidden:tu&quot;);ctx._source.remove(&quot;*hidden:tu__STRING&quot;);] using lang [painless], caused_by=&#123;type=circuit_breaking_exception, reason=[script] Too many dynamic script compilations within one minute, max: [15/min]; please use on-disk, indexed, or scripts with parameters instead; this limit can be changed by the [script.max_compilations_per_minute] setting, bytes_wanted=0, bytes_limit=0&#125;&#125;&#125;]\n    at org.janusgraph.diskstorage.es.rest.RestElasticSearchClient.bulkRequest(RestElasticSearchClient.java:258)\n    at org.janusgraph.diskstorage.es.ElasticSearchIndex.mutate(ElasticSearchIndex.java:601)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction$1.call(IndexTransaction.java:160)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction$1.call(IndexTransaction.java:157)\n    at org.janusgraph.diskstorage.util.BackendOperation.executeDirect(BackendOperation.java:69)\n    at org.janusgraph.diskstorage.util.BackendOperation.execute(BackendOperation.java:55)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction.flushInternal(IndexTransaction.java:157)\n    at org.janusgraph.diskstorage.indexing.IndexTransaction.commit(IndexTransaction.java:138)\n    at org.janusgraph.diskstorage.BackendTransaction.commitIndexes(BackendTransaction.java:141)\n    at org.janusgraph.graphdb.database.StandardJanusGraph.commit(StandardJanusGraph.java:765)\n    at org.janusgraph.graphdb.transaction.StandardJanusGraphTx.commit(StandardJanusGraphTx.java:1374)\n    at org.janusgraph.graphdb.tinkerpop.JanusGraphBlueprintsGraph$GraphTransaction.doCommit(JanusGraphBlueprintsGraph.java:272)\n    at org.apache.tinkerpop.gremlin.structure.util.AbstractTransaction.commit(AbstractTransaction.java:105)\n    at org.apache.tinkerpop.gremlin.structure.Transaction$commit$4.call(Unknown Source)\n    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48)\n    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113)\n    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117)\n    at Script27489.run(Script27489.groovy:1)\n    at org.apache.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:843)\n    at org.apache.tinkerpop.gremlin.groovy.jsr223.GremlinGroovyScriptEngine.eval(GremlinGroovyScriptEngine.java:548)\n    at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:233)\n    at org.apache.tinkerpop.gremlin.groovy.engine.ScriptEngines.eval(ScriptEngines.java:120)\n    at org.apache.tinkerpop.gremlin.groovy.engine.GremlinExecutor.lambda$eval$0(GremlinExecutor.java:290)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)该错误主要由于某段时间编译了大量的脚本,导致触发了ES的保护机制,拒绝了部分脚本的编译请求.\n最简单的解决方案就是调大限制,但是不可避免的会给ES集群造成压力,因此,不采用当前方案;另一种方案就是想办法从根本上解决大量脚本编译的问题.\n在我们的使用场景下,通常在大量删除节点的属性(相关属性创建了MixedIndex索引)时会出现这个问题,对具体的场景进行分析发现,当我们在删除节点属性时,JanusGraph\n会向ES发送fields删除的脚本.首先,这部分脚本没有进行参数化;其次,JanusGraph\n构造脚本时,属性列表顺序并不是有序的,因此,虽然我们的使用场景中每次删除的都是不同节点上的相同属性,但可能每次执行删除操作时构造的脚本都不同,发送至ES后,ES每次都需重新编译,所以触发了这个问题.\n\n在JanusGraph0.5.x版本后,修复了这个问题,删除Fields时,使用的是参数化的语句,且脚本已经提前提交至ES进行了编译.对于该问题的优化相关的解释可参考\nElasticsearch Painless script编程或ElasticSearch-DynamicScript使用总结\n但是很可惜,我们的使用场景中无法升级JanusGraph版本至0.5.x,因为该版本以后JanusGraph不在支持ES5,我们的项目还不能升级ES\n版本,因此只能对之前的JanusGraph的代码进行修改.相关的修改如下:\n\n思路(参考JanusGraph0.5.x版本中的修复方案):\n\n\n服务启动时,尝试存储可参数化的脚本至ES集群中.\n使用脚本时,通过参数化的方式发送请求至ES集群,可以最有效的防止频繁进行脚本的编译.\n\n\n代码修改时主要关注ElasticSearchIndex即可(其他已省略),可通过该类逐步完成所有修改.\n\njava\n    // 添加需要执行的脚本, ElasticSearchIndex.java\n    private static final String PARAMETERIZED_DELETION_SCRIPT = parameterizedScriptPrepare(&quot;&quot;,\n        &quot;for (field in params.fields) &#123;&quot;,\n        &quot;    if (field.cardinality == &#39;SINGLE&#39;) &#123;&quot;,\n        &quot;        ctx._source.remove(field.name);&quot;,\n        &quot;    &#125; else if (ctx._source.containsKey(field.name)) &#123;&quot;,\n        &quot;        def fieldIndex = ctx._source[field.name].indexOf(field.value);&quot;,\n        &quot;        if (fieldIndex &gt;= 0 &amp;&amp; fieldIndex &lt; ctx._source[field.name].size()) &#123;&quot;,\n        &quot;            ctx._source[field.name].remove(fieldIndex);&quot;,\n        &quot;        &#125;&quot;,\n        &quot;    &#125;&quot;,\n        &quot;&#125;&quot;);\n\n    private static final String PARAMETERIZED_ADDITION_SCRIPT = parameterizedScriptPrepare(&quot;&quot;,\n        &quot;for (field in params.fields) &#123;&quot;,\n        &quot;    if (ctx._source[field.name] == null) &#123;&quot;,\n        &quot;        ctx._source[field.name] = [];&quot;,\n        &quot;    &#125;&quot;,\n        &quot;    if (field.cardinality != &#39;SET&#39; || ctx._source[field.name].indexOf(field.value) == -1) &#123;&quot;,\n        &quot;        ctx._source[field.name].add(field.value);&quot;,\n        &quot;    &#125;&quot;,\n        &quot;&#125;&quot;);\n...\n    // 构造方法是重点,此处会将painless脚本通过_script端点存储至ES集群中\n    public ElasticSearchIndex(Configuration config) throws BackendException &#123;\n        indexName = config.get(INDEX_NAME);\n        parameterizedAdditionScriptId = generateScriptId(&quot;add&quot;);\n        parameterizedDeletionScriptId = generateScriptId(&quot;del&quot;);\n        useAllField = config.get(USE_ALL_FIELD);\n        useExternalMappings = config.get(USE_EXTERNAL_MAPPINGS);\n        allowMappingUpdate = config.get(ALLOW_MAPPING_UPDATE);\n        createSleep = config.get(CREATE_SLEEP);\n        ingestPipelines = config.getSubset(ES_INGEST_PIPELINES);\n        final ElasticSearchSetup.Connection c = interfaceConfiguration(config);\n        client = c.getClient();\n\n        batchSize = config.get(INDEX_MAX_RESULT_SET_SIZE);\n        log.debug(&quot;Configured ES query nb result by query to &#123;&#125;&quot;, batchSize);\n\n        switch (client.getMajorVersion()) &#123;\n            case FIVE:\n                compat = new ES5Compat();\n                break;\n            case SIX:\n                compat = new ES6Compat();\n                break;\n            default:\n                throw new PermanentBackendException(&quot;Unsupported Elasticsearch version: &quot; + client.getMajorVersion());\n        &#125;\n\n        try &#123;\n            client.clusterHealthRequest(config.get(HEALTH_REQUEST_TIMEOUT));\n        &#125; catch (final IOException e) &#123;\n            throw new PermanentBackendException(e.getMessage(), e);\n        &#125;\n        if (!config.has(USE_DEPRECATED_MULTITYPE_INDEX) &amp;&amp; client.isIndex(indexName)) &#123;\n            // upgrade scenario where multitype index was the default behavior\n            useMultitypeIndex = true;\n        &#125; else &#123;\n            useMultitypeIndex = config.get(USE_DEPRECATED_MULTITYPE_INDEX);\n            Preconditions.checkArgument(!useMultitypeIndex || !client.isAlias(indexName),\n                    &quot;The key &#39;&quot; + USE_DEPRECATED_MULTITYPE_INDEX\n                    + &quot;&#39; cannot be true when existing index is split.&quot;);\n            Preconditions.checkArgument(useMultitypeIndex || !client.isIndex(indexName),\n                    &quot;The key &#39;&quot; + USE_DEPRECATED_MULTITYPE_INDEX\n                    + &quot;&#39; cannot be false when existing index contains multiple types.&quot;);\n        &#125;\n        indexSetting = new HashMap&lt;&gt;();\n\n        ElasticSearchSetup.applySettingsFromJanusGraphConf(indexSetting, config);\n        indexSetting.put(&quot;index.max_result_window&quot;, Integer.MAX_VALUE);\n        \n        // 通过该方法在服务启动时存储脚本,注意存储脚本时需要注意版本兼容性问题\n        setupStoredScripts();\n    &#125;\n\n    // 该方法是执行请求的路口,在该方法中需要将原脚本修改为参数化脚本,防止触发ES保护机制\n    @Override\n    public void mutate(Map&lt;String, Map&lt;String, IndexMutation&gt;&gt; mutations, KeyInformation.IndexRetriever information,\n                       BaseTransaction tx) throws BackendException &#123;\n        final List&lt;ElasticSearchMutation&gt; requests = new ArrayList&lt;&gt;();\n        try &#123;\n            for (final Map.Entry&lt;String, Map&lt;String, IndexMutation&gt;&gt; stores : mutations.entrySet()) &#123;\n                final List&lt;ElasticSearchMutation&gt; requestByStore = new ArrayList&lt;&gt;();\n                final String storeName = stores.getKey();\n                final String indexStoreName = getIndexStoreName(storeName);\n                for (final Map.Entry&lt;String, IndexMutation&gt; entry : stores.getValue().entrySet()) &#123;\n                    final String documentId = entry.getKey();\n                    final IndexMutation mutation = entry.getValue();\n                    assert mutation.isConsolidated();\n                    Preconditions.checkArgument(!(mutation.isNew() &amp;&amp; mutation.isDeleted()));\n                    Preconditions.checkArgument(!mutation.isNew() || !mutation.hasDeletions());\n                    Preconditions.checkArgument(!mutation.isDeleted() || !mutation.hasAdditions());\n                    //Deletions first\n                    if (mutation.hasDeletions()) &#123;\n                        if (mutation.isDeleted()) &#123;\n                            log.trace(&quot;Deleting entire document &#123;&#125;&quot;, documentId);\n                            requestByStore.add(ElasticSearchMutation.createDeleteRequest(indexStoreName, storeName,\n                                    documentId));\n                        &#125; else &#123;\n//                            final String script = getDeletionScript(information, storeName, mutation);\n//                            final Map&lt;String,Object&gt; doc = compat.prepareScript(script).build();\n                            List&lt;Map&lt;String, Object&gt;&gt; params = getParameters(information.get(storeName),\n                                mutation.getDeletions(), true);\n                            Map doc = compat.prepareStoredScript(parameterizedDeletionScriptId, params).build();\n                            log.trace(&quot;Deletion script &#123;&#125; with params &#123;&#125;&quot;, PARAMETERIZED_DELETION_SCRIPT, params);\n                            requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n                                    documentId, doc));\n                        &#125;\n                    &#125;\n                    if (mutation.hasAdditions()) &#123;\n                        if (mutation.isNew()) &#123; //Index\n                            log.trace(&quot;Adding entire document &#123;&#125;&quot;, documentId);\n                            final Map&lt;String, Object&gt; source = getNewDocument(mutation.getAdditions(),\n                                    information.get(storeName));\n                            requestByStore.add(ElasticSearchMutation.createIndexRequest(indexStoreName, storeName,\n                                    documentId, source));\n                        &#125; else &#123;\n                            final Map upsert;\n                            if (!mutation.hasDeletions()) &#123;\n                                upsert = getNewDocument(mutation.getAdditions(), information.get(storeName));\n                            &#125; else &#123;\n                                upsert = null;\n                            &#125;\n\n                            List&lt;Map&lt;String, Object&gt;&gt; params = getParameters(information.get(storeName),\n                                mutation.getAdditions(), false, Cardinality.SINGLE);\n                            if (!params.isEmpty()) &#123;\n                                ImmutableMap.Builder builder = compat.prepareStoredScript(parameterizedAdditionScriptId, params);\n                                requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n                                    documentId, builder, upsert));\n                                log.trace(&quot;Adding script &#123;&#125; with params &#123;&#125;&quot;, PARAMETERIZED_ADDITION_SCRIPT, params);\n                            &#125;\n//                            final String inline = getAdditionScript(information, storeName, mutation);\n//                            if (!inline.isEmpty()) &#123;\n//                                final ImmutableMap.Builder builder = compat.prepareScript(inline);\n//                                requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n//                                        documentId, builder, upsert));\n//                                log.trace(&quot;Adding script &#123;&#125;&quot;, inline);\n//                            &#125;\n\n                            final Map&lt;String, Object&gt; doc = getAdditionDoc(information, storeName, mutation);\n                            if (!doc.isEmpty()) &#123;\n                                final ImmutableMap.Builder builder = ImmutableMap.builder().put(ES_DOC_KEY, doc);\n                                requestByStore.add(ElasticSearchMutation.createUpdateRequest(indexStoreName, storeName,\n                                        documentId, builder, upsert));\n                                log.trace(&quot;Adding update &#123;&#125;&quot;, doc);\n                            &#125;\n                        &#125;\n                    &#125;\n                &#125;\n                if (!requestByStore.isEmpty() &amp;&amp; ingestPipelines.containsKey(storeName)) &#123;\n                    client.bulkRequest(requestByStore, String.valueOf(ingestPipelines.get(storeName)));\n                &#125; else if (!requestByStore.isEmpty()) &#123;\n                    requests.addAll(requestByStore);\n                &#125;\n            &#125;\n            if (!requests.isEmpty()) &#123;\n                client.bulkRequest(requests, null);\n            &#125;\n        &#125; catch (final Exception e) &#123;\n            log.error(&quot;Failed to execute bulk Elasticsearch mutation&quot;, e);\n            throw convert(e);\n        &#125;\n    &#125;由于存储脚本的API在不同的版本中存在变化,因此需要对此部分做兼容处理,相关变化详见elasticsearch scripting module\n","slug":"janusgraph-index-problem","date":"2020-06-14T06:00:00.000Z","categories_index":"janusgraph,index","tags_index":"janusgraph,index","author_index":"Rod"}]